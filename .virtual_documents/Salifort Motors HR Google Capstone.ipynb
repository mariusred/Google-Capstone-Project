











# Import packages
### YOUR CODE HERE ### 
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt 
import seaborn as sns

pd.set_option('display.max_columns', None)

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score,\
f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report,\
roc_auc_score, roc_curve
from sklearn.tree import plot_tree

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans

from xgboost import XGBClassifier
from xgboost import XGBRegressor
from xgboost import plot_importance


# RUN THIS CELL TO IMPORT YOUR DATA. 

# Load dataset into a dataframe
### YOUR CODE HERE ###
df0 = pd.read_csv("HR_capstone_dataset.csv")


# Display first few rows of the dataframe
### YOUR CODE HERE ###
df0.head()








# Gather basic information about the data
### YOUR CODE HERE ###
df0.dtypes





# Gather descriptive statistics about the data
### YOUR CODE HERE ###
df0.describe()








# Display all column names
### YOUR CODE HERE ###
df0.columns


# Rename columns as needed
### YOUR CODE HERE ###

df0 = df0.rename(columns = {'Work_accident': 'work_accident','Department':'department','time_spend_company':'tenure','average_montly_hours':'average_monthly_hours'})
# Display all column names after the update
### YOUR CODE HERE ###
df0.columns








# Check for missing values
### YOUR CODE HERE ###
df0.isna().sum()








# Check for duplicates
### YOUR CODE HERE ###
dupes = df0.duplicated()


# Inspect some rows containing duplicates as needed
### YOUR CODE HERE ###
df0.loc[dupes]


# Drop duplicates and save resulting dataframe in a new variable as needed
### YOUR CODE HERE ###
df0.drop_duplicates(inplace = True, keep = 'first')

# Display first few rows of new dataframe as needed
### YOUR CODE HERE ###
df0.head()








# Create a boxplot to visualize distribution of `tenure` and detect any outliers
### YOUR CODE HERE ###
sns.boxplot(x = 'tenure', data = df0)


# Determine the number of rows containing outliers
### YOUR CODE HERE ###

q75 = df0['tenure'].quantile(0.75)
q25 = df0['tenure'].quantile(0.25)
iqr = q75 - q25

lower_limit = q25 - (1.5*iqr)
upper_limit = q75 + (1.5*iqr)

print('Lower limit: ','%.2f' % lower_limit)
print('Upper limit: ','%.2f' % upper_limit)

outliers = df0[(df0['tenure'] > upper_limit) | (df0['tenure'] < lower_limit)]

print('Number of outliers in tenure: ',len(outliers))

sns.boxplot(x = 'tenure', data = df0)
plt.axvline(x = upper_limit, color = 'red', linestyle = '--')
plt.axvline(x = lower_limit, color = 'red', linestyle = '--')
plt.show()











# Get numbers of people who left vs. stayed
### YOUR CODE HERE ###
print(df0['left'].value_counts())
# Get percentages of people who left vs. stayed
### YOUR CODE HERE ###
print(df0['left'].value_counts(normalize = True)*100)








# Create a plot as needed
### YOUR CODE HERE ###
###Check for the satisfaction level of employees who left and their average working hours.
plt.figure(figsize=(16,9))
sns.scatterplot(data = df0, y = 'satisfaction_level', x = 'average_monthly_hours', hue = 'left',alpha = 0.4)
plt.legend(labels = ['left','stayed'])
plt.title('Satisfaction of Employees vs Average Monthly work hours')





# Create a plot as needed
### YOUR CODE HERE ###
###Check the departments of those who left the company
plt.figure(figsize=(16,9))
sns.scatterplot(data = df0, y = 'satisfaction_level', x = 'average_monthly_hours', hue = 'department',alpha = 0.4)
#plt.legend(labels = ['left','stayed'])
plt.title('Satisfaction of Employees vs Average Monthly work hours')





# Create a plot as needed
### YOUR CODE HERE ###
### Investigate the evaluation scores of those who left according to their average monthly hours.
plt.figure(figsize=(16,9))
sns.scatterplot(data = df0, y = 'last_evaluation', x = 'average_monthly_hours', hue = 'left',alpha = 0.4)
plt.legend(labels = ['left','stayed'])
plt.axvline(x = 160, color = 'red', linestyle = '--')
plt.axvline(x = 215, color = 'red', linestyle = '--')
plt.axvline(x = 310, color = 'red', linestyle = '--')
plt.title('Latest Evaluation Score of Employees vs Average Monthly work hours')





# Create a plot as needed
### YOUR CODE HERE ###
fig, ax = plt.subplots(1, 2, figsize = (22,8))
sns.histplot(x = 'number_project', data = df0, hue = 'left', multiple = 'dodge', ax = ax[0])
ax[0].set_title('Histogram for Number of projects')

sns.boxplot(data = df0, x = 'average_monthly_hours', y = 'number_project', hue = 'left', orient = 'h', ax = ax[1])
ax[1].axvline(x = 160, color = 'red', linestyle = '--')
ax[1].invert_yaxis()
ax[1].set_title('Box plot of Average monthly hours to number of projects')





# Create a plot as needed
### YOUR CODE HERE ###
plt.figure(figsize=(16,3))
sns.scatterplot(data = df0, y = 'promotion_last_5years', x = 'average_monthly_hours', hue = 'left',alpha = 0.4)
plt.legend(labels = ['left','stayed'])
plt.axvline(x = 160, color = 'red', linestyle = '--')
plt.title('Promotion status vs Average Monthly work hours')





# Create a plot as needed
### YOUR CODE HERE ###
fig, ax = plt.subplots(1, 2, figsize = (22,8))
sns.histplot(x = 'tenure', data = df0, hue = 'left', multiple = 'dodge', shrink = 5, ax = ax[0])
ax[0].set_title('Histogram for Employee Tenure')

sns.boxplot(data = df0, x = 'satisfaction_level', y = 'tenure', hue = 'left', orient = 'h', ax = ax[1])
ax[1].invert_yaxis()
ax[1].set_title('Box plot of Employee Satisfation by Tenure')





# Create a plot as needed
### YOUR CODE HERE ###
fig,ax = plt.subplots(1,2, figsize = (22,8))
tenure_short = df0[df0['tenure'] <= 6]
tenure_long = df0[df0['tenure'] > 6]

sns.histplot(data = tenure_short, x = 'tenure', hue = 'salary', hue_order = ['low','medium', 'high'], multiple = 'dodge', shrink = 5, ax = ax[0])
ax[0].set_title('Salary histogram of short-tenured employees')

sns.histplot(data = tenure_long, x = 'tenure', hue = 'salary', hue_order = ['low','medium', 'high'], multiple = 'dodge', ax = ax[1])
ax[1].set_title('Salary histogram of long-tenured employees')


print('Salary percentage of short-tenured employees')
print(tenure_short['salary'].value_counts(normalize = True)*100)
print('Salary percentage of long-tenured employees')
print(tenure_long['salary'].value_counts(normalize = True)*100)





df_encoded = df0.copy()
df_encoded['salary'] = (df_encoded['salary'].astype('category').cat.set_categories(['low','medium','high']).cat.codes)
df_encoded['salary'].value_counts()





# Create a plot as needed
### YOUR CODE HERE ###
plt.figure(figsize=(16,9))
heatmap = sns.heatmap(df_encoded.corr(numeric_only = True), vmin = 1, vmax = -1, annot = True, cmap=sns.color_palette("vlag", as_cmap = True))
heatmap.set_title("Correlation Heatmap", fontdict ={'fontsize':14}, pad = 12);
































### YOUR CODE HERE ###
df_encoded = pd.get_dummies(df_encoded, drop_first = False)
df_encoded.head()


plt.figure(figsize = (8,6))
sns.heatmap(df_encoded[['satisfaction_level','last_evaluation','number_project','average_monthly_hours','tenure']].corr(), annot = True, cmap = "crest")
plt.title('Dataset Heatmap')
plt.show





df_logreg = df_encoded.copy()
df_logreg = df_logreg[(df_logreg['tenure'] <= upper_limit) & (df_logreg['tenure'] >= lower_limit)]
print('Before:', df_encoded.shape)
print('After:',df_logreg.shape)


y_logreg = df_logreg['left']
X_logreg = df_logreg.copy()
X_logreg = df_logreg.drop('left', axis = 1)


X_train, X_test, y_train, y_test = train_test_split(X_logreg, y_logreg, stratify = y_logreg, test_size = 0.25, random_state =42)


logreg_clf = LogisticRegression(random_state = 42, max_iter = 500).fit(X_train, y_train)


y_pred = logreg_clf.predict(X_test)


logreg_cm = confusion_matrix(y_test,y_pred, labels = logreg_clf.classes_)

logreg_disp = ConfusionMatrixDisplay(confusion_matrix = logreg_cm, display_labels = logreg_clf.classes_)

logreg_disp.plot(values_format = '')

plt.show


target_names = ['Predicted would not leave', 'Predicted would leave']
print(classification_report(y_test,y_pred, target_names = target_names))


df_logreg['left'].value_counts(normalize = True) * 100








y_dectree = df_encoded["left"]
y_dectree.head()


X_dectree = df_encoded.copy()
X_dectree = df_encoded.drop('left', axis = 1)
X_dectree.head()


X_train, X_test, y_train, y_test = train_test_split(X_dectree, y_dectree, stratify = y_dectree, test_size = 0.25, random_state =0)


tree = DecisionTreeClassifier(random_state = 0)

cv_params = {'max_depth': [4,6,8, None],
             'min_samples_leaf': [2,5,1],
             'min_samples_split': [2,4,6]
            }

scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

tree1 = GridSearchCV(tree, cv_params, scoring = scoring , cv = 4, refit = 'roc_auc')


%%time
dectree_clf = tree1.fit(X_train, y_train)


dectree_ypred = dectree_clf.predict(X_test)


dectree_cm = confusion_matrix(y_test,dectree_ypred, labels = dectree_clf.classes_)

dectree_disp = ConfusionMatrixDisplay(confusion_matrix = dectree_cm, display_labels = dectree_clf.classes_)

dectree_disp.plot(values_format = '')

plt.show


print('Best AUC Score:',tree1.best_score_)
print('Optimal parameters:', tree1.best_params_)


def make_results(model_name:str, model_object, metric:str):
    '''
    Arguments:
        model_name (string): what you want the model to be called in the output table
        model_object: a fit GridSearchCV object
        metric (string): precision, recall, f1, accuracy, or auc
  
    Returns a pandas df with the F1, recall, precision, accuracy, and auc scores
    for the model with the best mean 'metric' score across all validation folds.  
    '''

    # Create dictionary that maps input metric to actual metric name in GridSearchCV
    metric_dict = {'auc': 'mean_test_roc_auc',
                   'precision': 'mean_test_precision',
                   'recall': 'mean_test_recall',
                   'f1': 'mean_test_f1',
                   'accuracy': 'mean_test_accuracy'
                  }

    # Get all the results from the CV and put them in a df
    cv_results = pd.DataFrame(model_object.cv_results_)

    # Isolate the row of the df with the max(metric) score
    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]

    # Extract Accuracy, precision, recall, and f1 score from that row
    auc = best_estimator_results.mean_test_roc_auc
    f1 = best_estimator_results.mean_test_f1
    recall = best_estimator_results.mean_test_recall
    precision = best_estimator_results.mean_test_precision
    accuracy = best_estimator_results.mean_test_accuracy
  
    # Create table of results
    table = pd.DataFrame()
    table = pd.DataFrame({'model': [model_name],
                          'precision': [precision],
                          'recall': [recall],
                          'F1': [f1],
                          'accuracy': [accuracy],
                          'AUC': [auc]
                        })
  
    return table


tree1_cv_results = make_results('decision tree cv', tree1, 'auc')
tree1_cv_results





#Do not execute, just load pickle file "hr_rf1" for model fitting results
'''
rf = RandomForestClassifier(random_state=0)
cv_params = {'max_depth': [3,5, None], 
             'max_features': [1.0],
             'max_samples': [0.7, 1.0],
             'min_samples_leaf': [1,2,3],
             'min_samples_split': [2,3,4],
             'n_estimators': [300, 500],
             }  
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
rf1 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='roc_auc')
'''


def write_pickle(path, model_object, save_as:str):
    '''
    In: 
        path:         path of folder where you want to save the pickle
        model_object: a model you want to pickle
        save_as:      filename for how you want to save the model

    Out: A call to pickle the model in the folder indicated
    '''    

    with open(path + save_as + '.pickle', 'wb') as to_write:
        pickle.dump(model_object, to_write)


def read_pickle(path, saved_model_name:str):
    '''
    In: 
        path:             path to folder where you want to read from
        saved_model_name: filename of pickled model you want to read in

    Out: 
        model: the pickled model 
    '''
    with open(path + saved_model_name + '.pickle', 'rb') as to_read:
        model = pickle.load(to_read)

    return model


#Do not execute, just load pickle file "hr_rf1" for model fitting results
'''
%%time
rf1.fit(X_train, y_train)
'''


import pickle
path = 'C:/Users/Ed/Documents/GitHub/Google-Capstone-Project/'
rf1 = read_pickle(path, 'hr_rf1')


rf1_ypred = rf1.predict(X_test)


rf1_cm = confusion_matrix(y_test,rf1_ypred, labels = rf1.classes_)

rf1_disp = ConfusionMatrixDisplay(confusion_matrix = rf1_cm, display_labels = rf1.classes_)

rf1_disp.plot(values_format = '')

plt.show


rf1.best_score_


rf1.best_params_


rf1_cv_results = make_results('random forest cv', rf1, 'auc')
print(tree1_cv_results)
print(rf1_cv_results)


def get_scores(model_name:str, model, X_test_data, y_test_data):
    '''
    Generate a table of test scores.

    In: 
        model_name (string):  How you want your model to be named in the output table
        model:                A fit GridSearchCV object
        X_test_data:          numpy array of X_test data
        y_test_data:          numpy array of y_test data

    Out: pandas df of precision, recall, f1, accuracy, and AUC scores for your model
    '''

    preds = model.best_estimator_.predict(X_test_data)

    auc = roc_auc_score(y_test_data, preds)
    accuracy = accuracy_score(y_test_data, preds)
    precision = precision_score(y_test_data, preds)
    recall = recall_score(y_test_data, preds)
    f1 = f1_score(y_test_data, preds)

    table = pd.DataFrame({'model': [model_name],
                          'precision': [precision], 
                          'recall': [recall],
                          'F1': [f1],
                          'accuracy': [accuracy],
                          'AUC': [auc]
                         })
  
    return table


rf1_test_scores = get_scores('random forest1 test', rf1, X_test, y_test)
print(rf1_test_scores,"\n" * 2, tree1_cv_results)


fig,ax = plt.subplots(1,3, figsize = (22,6))
logreg_disp.plot(values_format = '', ax = ax[0])
ax[0].set_title('Logistic Regression Confusion Matrix')

rf1_disp.plot(values_format = '', ax = ax[1])
ax[1].set_title('Random Forest Confusion Matrix')

dectree_disp.plot(values_format = '',ax = ax[2])
ax[2].set_title('Decision Tree Confusion Matrix')


















